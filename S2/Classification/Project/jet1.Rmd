---
title: "Cyberattaques des smartphones"
author: "Loïc Rakotoson"
output:
  html_notebook:
    df_print: paged
    highlight: tango
    theme: cerulean
  pdf_document: default
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.1'
      jupytext_version: 1.2.4
  kernelspec:
    display_name: R
    language: R
    name: ir
---

```{r message=FALSE, warning=FALSE}
library(ggpubr)
library(tidyverse)
library(caret)
library(pROC)
library(ranger)
library(doParallel)
```

# Introduction
L'objet de l'étude est la **classification des cyberattaques** sur les appareils mobiles sous sytème Android à partir des données qui s'échangent et transitent par le smartphone, recoltées par le moniteur des ressources. Le but étant d'effectuer le traitement adapté pour nettoyer l'appareil sans avoir besoin de la réinitialiser en perdant des données personnelles.

Pour classer des appareils infectées ou non, les données du moniteur de ressources ne permettent pas de le faire puisque le téléphone attaqué arrive à avoir un comportement "normal" (sauf quelques `ransomeware`). L'analyse des logs (texte) permet de déceler les moments anormaux mais ne renseigne pas le type d'infection. Donc pour un log de réseau anormal, on récolte les données du moniteur de ressources. Nous intervenons dans cette deuxième partie.  
Les données proviennent de l'[Université du Nouveau-Brunswick](https://www.unb.ca/cic/datasets/index.html) au Canada.

Dans cette étude, l'appareil est déjà infectée et les attaques ne peuvent pas se superposer (impossibilité d'avoir plus d'une infection).  
En regroupant certaines classes de même nature, dont les variances sont très faibles, on a 4 attaques qui sont indécelables en analysant le comportement de l'appareil de façon générale:

- `adware`, qui affiche des pubs ciblées tout en espionnant de manière illégale les activités de l'appareil.
- `botnet`, qui intègre le téléphone à un réseau d'[appareils zombies](https://fr.wikipedia.org/wiki/Machine_zombie).
- `ransomeware`, qui chiffre l'intégralité des données ou uniquement une partie (le cas ici) et qui se déchiffre après avoir payé une rançon ou en utilisant un décrypteur.
- `sms`, qui installe un vers ou un cheval de Troie pour le transformer en appareil zombie.

Pour être actifs, les appareils communiquent avec les serveurs des cyberattaquants. Nous analyserons les données liées à ces échanges.

```{r message=FALSE, warning=FALSE}
data <- read_csv("data/hackerAttack.csv")
data <- data %>% rename_all(make.names)
```

# I. Features Engineering
Dans nos données brutes, nous avons 81 variables et 4 000 observations.  
Pour nettoyer nos données, nous devons supprimer les colonnes constantes ou ayant de faible variance (inférieure au premier quartile des variances), transformer certaines variables:

- `Destination IP`, en facteur `IP` en ne gardant que le **Net-ID** (le 1<sup>er</sup> octet si classe A, les 2 premiers sinon)
- `Source Port` et `Destination Port`, en `SPort` et `DPort`, de num à facteur en suivant les types d'attribution de l'[IANA](https://fr.wikipedia.org/wiki/Internet_Assigned_Numbers_Authority)
- `Protocol`, de num à facteur en suivant les [assignements](https://www.iana.org/assignments/protocol-numbers/protocol-numbers.xhtml) de l'IANA
- `Label`, nos variables à expliquer $Y$ en facteur.
- Les variables quantitatives, standardisées.

```{r message=FALSE, warning=FALSE}
ip_split <- function(ip){
    #' Renvoie le Net-ID d'une adresse IP
    octets <- str_split(ip, "\\.")[[1]]
    if (as.numeric(octets[1]) <= 127) {
        netID <- octets[1]
    } else {
        netID <- paste(octets[1], octets[2], sep = ".")
    }
    
    return(netID)
}
```

```{r quantile, include=FALSE}
data %>% 
select_if(is.numeric) %>% 
summarise_all(var) %>% 
gather() %>%
select(value) %>%
as.matrix() %>%
quantile(0.25) -> q1
```

Transformation des IPs.

```{r message=FALSE, warning=FALSE}
data <- data %>% 
rename(IP = 'Destination.IP') %>% 
rowwise() %>% # vectorise la fonction ip_split
mutate(IP = ip_split(IP)) %>% 
as_tibble() # supprime rowwise_df
```

Les autres transformations

```{r message=FALSE, warning=FALSE}
data <- data %>% 
rename(
    SPort = 'Source.Port',
    DPort = 'Destination.Port',
) %>% 
mutate(
    DPort = case_when(
        DPort == 53 ~ "DNS",
        DPort == 80 ~ "HTTP",
        DPort == 443 ~ "HTTPS",
        DPort %in% c(1024:5000, 49152:65535) ~ "Microsoft RPC",
        DPort %in% 60000:61000 ~ "Mosh",
        TRUE ~ "Private"
    ),
    SPort = case_when(
        SPort == 80 ~ "HTTP",
        SPort == 443 ~ "HTTPS",
        SPort %in% c(1024:5000, 49152:65535) ~ "Microsoft RPC",
        SPort %in% 60000:61000 ~ "Mosh",
        TRUE ~ "Private"
    )
) %>% 
mutate_at(
    c('SPort', 'DPort', 'IP', 'Protocol', 'Label'),
    funs(factor)
) %>% 
select(
    1:4, # Facteurs
    colnames(select_if(., function(col) is.numeric(col) && var(col) >= q1)), # Variances faibles
    ncol(.) # Label
) %>% 
mutate_if(
    is.numeric,
    scale
)
```


```{r include=FALSE}
dim_quant <- data %>% select_if(is.numeric) %>% ncol()
dim_IP <- length(unique(data$IP))
dim_SPort <- length(unique(data$SPort))
dim_DPort <- length(unique(data$DPort))
dim_Protocol <- length(unique(data$Protocol))
dim_X <- dim_quant + dim_IP-1 + dim_SPort-1 + dim_DPort-1 + dim_Protocol-1
```

Nous obtenons une dimension de `r dim(data)` après nettoyage.

Un total de `r dim_X` variables explicatives:

- `r dim_quant` variables quantitatives.
- `r dim_IP` modalités de IP.
- `r dim_SPort` modalités de SPort
- `r dim_DPort` modalités de DPort
- `r dim_Protocol` modalités de Protocol.

On ne peut pas utiliser l'ACP pour réduire les dimensions puisque les variables ne sont pas linéaires. La stratégie sera d'éliminer les variables qui ne sont pas importantes pour les modèles de classifications.


# II. Statistique descriptive
Visualisons dans cette partie la distribution de nos variables pour en tirer des informations qui peuvent aider à la classification.

## 1. Variables qualitatives

### Les Ports et les attaques
En visualisant la répartition de l'utilisation des ports d'entrée et de sortie pour chaque infection, on peut voir:

- De manière générale, les attaquants récoltent les données ou les réponses de leurs requêtes en ouvrant des ports privés ou en utilisant l'exploit Microsoft RPC. Ils les recoivent via des requêtes HTTP et HTTPS sur leurs serveurs distants. L'étude des IP permettra d'identifier les serveurs ou les 1<sup>er</sup> noeuds s'il s'agit d'un [réseau TOR](https://fr.wikipedia.org/wiki/Tor_(r%C3%A9seau)).
- Le port DNS n'est pas utilisé en source et très peu pour recevoir. Les attaques par DNS les plus courants sont des [DDoS](https://fr.wikipedia.org/wiki/Attaque_par_d%C3%A9ni_de_service) contre de grandes structures ou du [Phishing](https://fr.wikipedia.org/wiki/Hame%C3%A7onnage) contre des personnes ciblées comme dans notre cas. On souligne directement que le `botnet` n'utilise pas le DNS puisqu'il n'a pas besoin d'usurper un site pour infecter.
- Près de 80% de la recéption des requêtes des `ransomeware` et 65% des `botnet` passent par le port 443, HTTPS. Il s'agit aussi des deux infections qui ont obligatoirement besoin d'un [back door](https://fr.wikipedia.org/wiki/Porte_d%C3%A9rob%C3%A9e) pour que l'attaquant garde le plus longtemps possible le contrôle de l'appareil.

```{r message=FALSE, warning=FALSE, fig.width=9.5}
data %>%
select(SPort, DPort, Label) %>%
gather("Entry", "Port", -Label) %>% ggplot() + 
aes(x = Port, y = ..prop.., group = 1, fill = Label) +
geom_bar() + guides(fill = F) + 
facet_grid(cols = vars(Label), rows = vars(Entry)) +
labs(title = "Utilisation des Ports par infection", x = "Ports", y = "proportion") +
theme(
    axis.text.x = element_text(angle = 50, hjust = 1),
    plot.title = element_text(hjust = 0.5)
)
```

### Les IPs et les protocoles
L'analyse des NetID des adresses IP permet d'obtenir des informations sur le comportement des infections.  
En analysant les 2 NetID les plus utilisés par infection, on s'aperçoit que:

- Le 172.217 (classe B) est le plus utilisé, sauf pour les `sms` qui le placent en 2<sup>e</sup>.
- Le deuxième le plus fréquent est de classe A, le 10 pour toutes sauf pour les `ransomeware` avec le 119.

L'analyse des protocoles (*ici utilisés pour la réception DPort*) y ajoute plus d'informations:

- On retrouve les 2 mêmes NetID les plus fréquents pour le protocol 6 (TCP) et 17 (UDP)
- Le protocol 0 (HOPOPT) utilise deux NetID de classe A, dont le 8 qui représente 93.8% du traffic
- Le 10 représente 87.5% du traffic du protocol 17 (UDP), moins sécurisé que le TCP.
- Au moins 95% des traffics utilisent le protocol 6 (TCP) pour toutes infections sauf `sms`, qui est le plus grand utilisateur du protocol 17 (UDP).

```{r include=FALSE}
ip_label <- data %>% 
group_by(Label, IP) %>% 
summarise(proportion = n()) %>%
mutate(proportion = proportion*100 / sum(proportion)) %>% 
top_n(2, proportion) %>% 
ggplot() + aes(x = IP, y = proportion, fill = IP) +
geom_bar(stat="identity") + facet_grid(Label~.) +
guides(fill = F) + xlab("NetID")

ip_protocol <- data %>% 
group_by(Protocol, IP) %>% 
summarise(proportion = n()) %>%
mutate(proportion = proportion*100 / sum(proportion)) %>% 
top_n(2, proportion) %>% 
ggplot() + aes(x = IP, y = proportion, fill = IP) +
geom_bar(stat="identity") + facet_grid(Protocol~.) +
guides(fill = F) + xlab("NetID") +
scale_fill_manual(values = c("#f39c12", "#f8766d", "#619cff", "#8e44ad"))
```

```{r message=FALSE, warning=FALSE, fig.width=9.5}
data %>% 
group_by(Label, Protocol) %>% 
summarise(proportion = n()) %>%
mutate(proportion = proportion*100 / sum(proportion)) %>% 
spread(Protocol, proportion)
ggarrange(
    ip_label, ip_protocol,
    ncol = 2, nrow = 1
) %>% 
annotate_figure(top = "Top 2 des NetIDs par infection et par protocole")
```

### En résumé
Avec l'analyse des 4 variables qualitatives, il est déjà possible de dresser les profils des infections:

- `sms` utilise moins de chiffrement: plutôt HTTP que HTTPS, avec l'utilisation du protocol 17 (UDP) qui reçoit beaucoup sur le NetID 10. Il s'agit d'une attaque courte. La variable `Flow Duration` devrait avoir une valeur faible et les `Packets` grandes.
- `ransomeware` est la plus chiffrée et reçoit sur le NetID 119 pour la classe A. Il s'agit de l'attaque communiquer le plus avec le serveur tant qu'elle n'est pas résolue. Les variables de types `Active` devrait prendre de grandes valeurs.
- `botnet` et `adware` se distinguent uniquement par l'utilisation des chiffrements à la réception (`botnet` est plus HTTP). Les variables quantitatives ne permettent pas plus de les discriminer.

## 2. Variables quantitatives
Rappelons que les données ont été standardisées.

### Longueur de paquet
La variable `Min Packet Length` est celle qui est la plus corrélée aux labels. Elle représente la taille minimale du packet en prenant en compte l'en-tête.  
Comme attendu, la densité est plus répartie pour `sms`. La variable étant plus concentrée autour de la moyenne pour `ransomeware` et `botnet`.

```{r message=FALSE, warning=FALSE, fig.width=9.5}
ggplot(data) + aes(x = `Min.Packet.Length`, fill = Label) +
geom_density() + facet_grid(Label~., scales = "free_y") +
xlim(range(-1,5)) + guides(fill = F)
```

### Durée de débit
La visualisation de `Flow Duration` permet de visualiser le temps de connexion entre le serveur et l'appareil.  
L'infection par `sms` étant rapide et n'ayant pas systématiquement besoin d'un backdoor, elle se distingue facilement par une durée de débit faible par rapport aux autres infections.  
Cela confirme l'analyse sur les protocols où le chiffrement n'est pas nécessaire.

```{r message=FALSE, warning=FALSE, fig.width=9.5}
ggplot(data) + aes(y = `Flow.Duration`, x = Label, fill = Label) +
geom_violin() + ylim(range(-0.6:2.6)) + guides(fill = F) +
geom_boxplot(width = 0.07, fill = "#ecf0f1", outlier.size = -1)
```

### Durée entre 2 paquets d'un débit
La variable `Flow IAT Mean` représente la vitesse moyenne d'envoi de paquets, entre autres la durée entre l'envoi de 2 paquets dans un seul débit. Un Flow IAT faible est un débit plus rapide.  
L'infection `sms` doit envoyer dans une durée faible le maximum d'informations, tandis que pour les `botnet`, pour lesquelles les machines zombies sont souvent utilisées pour attaquer en DDoS, l'attaquant envoie une instruction et doit attendre l'exécution de celle-ci avant de déconnecter, d'où une vitesse faible.

```{r message=FALSE, warning=FALSE, fig.width=9.5}
ggplot(data) + 
aes(y = `Flow.IAT.Mean`, x = Label, fill = Label) +
geom_boxplot(notch=T, outlier.size = -1) +
coord_flip(ylim=c(-0.5, 1)) +
guides(fill = F)
```

# III. Benchmark et optimisation des modèles

Dans la suite, nous allons comparer 4 algorithmes de classifications :

- Les [KNN](#1.-K-plus-proches-voisins), faciles à mettre en oeuvre,
- Les [forêts aléatoires](#2.-For%C3%AAts-al%C3%A9atoires), classifieurs les plus efficaces "out-of-the-box",
- Le [logitboost](#3.-Logit-Boost), la version stochastique d'Adaboost grâce à la régression logistique,
- [Stochastic gradient boosting](#4.-Gradient-boosting), qui a montré son efficacité dans les challenges.

Nos données n'étant pas linéaires, les analyses discriminantes de type LDA ou QDA sont peu pertinantes.

### Stratégie
La stratégie sera de créer 4 folds dont 10% pour le test final et 3 x 30% pour les validations croisées en entraînement. Chacun des 4 algorithmes s'entraînera **sur les mêmes folds en validation croisée** (*avec les tests*), pour que la comparaison ait un sens, et sera testé en plus sur le 4<sup>e</sup> fold pour pouvoir tester sur des données pas encore vues et tracer les courbes ROC.  
L'optimisation des paramètres se fera par Grid Search en 2 temps, une approche naïve et une optimisation, en utilisant comme métriques l'**AUC** et l'**Accuracy**. L'évaluation et la sélection de modèle se fera en fonction des deux mêmes métriques.  
Pour se faire, 4 fonctions ont été codées (*ne figurent pas ici mais présentes dans le markdown associé*):

- `split_set( )`: divise nos données de manière aléatoire en partitions fixes train et test, en 90% - 10%
- `plotmetrics( )`: représente l'évolution de l'AUC et de l'Accuracy en fonction de l'évolution d'un paramètre à optimiser
- `confmatrix( )`: repésente la matrice de confusion
- `multiroc( )`: représente la courbe ROC d'un modèle pour chaque label en One-vs-rest

L'objet `tune_control` fixe les folds pour la validation croisée pour permettre l'évaluation avec nos métriques et la comparaison de nos modèles

```{r message=FALSE, warning=FALSE}
split_set <- function(data, trainSize = 0.9){
    #' Divise nos données en partitions train et test
    
    set.seed(2020)
    part <- createDataPartition(data$Label, p = trainSize)
    train_set <- data %>% slice(part$Resample1)
    test_set <- data %>% slice(-part$Resample1)
    
    return(list(train_set, test_set))
}
```

```{r include=FALSE}
plotmetrics <- function(train_obj, parameter){
    #' Renvoie un ggplot de l'évolution d'Accuracy et AUC
    #' train_obj: objet de type train de caret
    #' param: paramètre évalué
    
    train_obj <- train_obj$results %>% select(!!parameter, AUC, Accuracy)
    AUC_limits <- c(min(train_obj$AUC), max(train_obj$AUC))
    Accuracy_limits <- c(min(train_obj$Accuracy), max(train_obj$Accuracy))
    train_obj$Accuracy <- train_obj$Accuracy * (AUC_limits[2]/Accuracy_limits[2])
    limit <- c(min(train_obj[,2:3]), max(train_obj[,2:3]))
    maxAUC <- filter(train_obj, AUC == max(AUC))
    maxAccuracy <- filter(train_obj, Accuracy == max(Accuracy))

    plot <- train_obj %>% 
    ggplot() + aes_string(x = parameter) + 
    geom_line(aes(y = AUC), color = "#38ada9") +  
    geom_point(aes(y = AUC), color = "#38ada9") +
    geom_line(aes(y = Accuracy), color = "#e55039") +
    geom_point(aes(y = Accuracy), color = "#e55039") +
    geom_point(data = maxAUC, aes_string(x = parameter, y = "AUC"), color = "#38ada9", shape = 5, size = 3) +
    geom_point(data = maxAccuracy, aes_string(x = parameter, y = "Accuracy"), color = "#e55039", shape = 0, size = 3) +
    scale_y_continuous(
        limits = limit,
        sec.axis = sec_axis(~ . *Accuracy_limits[2]/AUC_limits[2],
                            name = "Accuracy")) +
    theme(
        plot.title = element_text(hjust = 0.5),
        axis.title.y.left = element_text(color = "#38ada9"),
        axis.text.y.left = element_text(color = "#38ada9"),
        axis.title.y.right = element_text(color = "#e55039"),
        axis.text.y.right = element_text(color = "#e55039"),

    )
    
    return(plot)
}
```

```{r}
confmatrix <- function(pred, test){
    #' Renvoie un ggplot de matrice de confusion
    #' pred, test: vecteurs de même taille
    
    plot <- confusionMatrix(pred, test)$table %>% 
    as_tibble() %>%
    ggplot() +
    aes(x = Reference, y = reorder(Prediction, desc(Prediction)), fill = n) + 
    geom_tile() +
    geom_text(aes(label = paste(round(n, 2), '%')), color = "white", size = 5) +
    labs(y = "Prediction") + guides(fill = F) +
    scale_fill_gradient(low = "#c0392b", high = "#27ae60") +
    theme(plot.title = element_text(hjust = 0.5))
    
    return(plot)
}
```

```{r message=FALSE, warning=FALSE}
multiroc <- function(Y_pred, Y){
    #' Renvoie les courbes ROC de chaque classe
    
    predicted <- tibble(
        Y = as.character(Y),
        Y_pred = as.character(Y_pred)
    )

    roclist <- vector(mode = "list")

    for (label in unique(predicted$Y)){
        one_rest <- predicted %>% 
        mutate(
            Y = if_else(
                Y == label,
                1, 0
            ),
            Y_pred = if_else(
                Y_pred == label,
                1, 0
            )
        )

        roclist[[label]] <- roc(one_rest$Y_pred, one_rest$Y)
    }

    plot <- roclist %>% ggroc(aes = "color", size = 1, legacy.axes = TRUE) +
    labs(color = "Labels") + theme(plot.title = element_text(hjust = 0.5))
    
    return(plot)
}

```

```{r message=FALSE, warning=FALSE}
train_set <- split_set(data)[[1]]
test_set <- split_set(data)[[2]]

tune_control <- trainControl(
  method = "cv", 
  number = 3,
  index = createMultiFolds(train_set$Label, k = 3, times = 1),
  classProbs = TRUE,
  summaryFunction = multiClassSummary
)
```

## 1. K plus proches voisins
L'algorithme des K plus proches voisins se base sur la distance entre les individus et ne fait aucune supposition préalable des données.  
Sa règle est la suivante:  
soient $k \leq n$, $x$ à classer et $\text{kkpv}(x) = \{i: X_i\, \text{parmi les}\, k \,\text{plus proches voisins de}\, x \,\text{dans}\, \{X_1, .. X_n\} \}$, la règle de classification est

$$\hat{g}(x) = \underset{k \in Y}{\operatorname{argmax}}  \underset{i \in kkpv(x)}{\operatorname{\sum}} 1_{Y_i = k} $$

### Approche naïve
Dans l'algorithme que nous allons entraîner, le paramètre à optimiser est $k$.  
Nous allons effectuer une recherche sur les 3 à 170 voisins par pas de 6. 

```{r message=FALSE, warning=FALSE, fig.width=9.5}
knn_tune_naive <- data.frame(
    k = seq(3, 170, by = 6)
)

cl <- makePSOCKcluster(4)
registerDoParallel(cl)
set.seed(2020)

knn_naive_cv <- train(
    Label ~ .,
    data = train_set,
    method = "knn",
    metric = "AUC",
    trControl = tune_control,
    tuneGrid = knn_tune_naive,
)

on.exit(stopCluster(cl))

plotmetrics(knn_naive_cv, "k") +
labs(title = "AUC et Accuracy en fonction des K voisins", x = "Voisins")
```

Sur la plage qu'on a testé, les meilleurs $k$ se situe entre 3 et 15, 3 pour Accuracy et 9 pour l'AUC.  
L'optimisation se fera donc sur cette nouvelle plage.

### Optimisation

Il n'y a pas de pre-traitement à faire en plus pour l'optimisation. Toutefois, pour l'uniformité des procédures de comparaison des modèles, affectons de nouveaux (*les mêmes*) sets. Ceci est utile pour la description du Pipeline si ce modèle sera choisi.

```{r message=FALSE, warning=FALSE}
train_knn <- split_set(data)[[1]]
test_knn <- split_set(data)[[2]]
```

En entraînant notre algorithme sur cette nouvelle plage de $k$ et en l'optimisant sur le critère de l'AUC, on obtient $k = 4$.  
Notre deuxième critère, l'Accuracy, est optimal pour $k = 3$, comme lors de l'approche naïve.  
Nous décidons de garder $k = 4$ en suivant notre premier critère.

```{r message=FALSE, warning=FALSE, fig.width=9.5}
knn_tune <- data.frame(
    k = seq(3, 15)
)

cl <- makePSOCKcluster(4)
registerDoParallel(cl)
set.seed(2020)

knn_cv <- train(
    Label ~ .,
    data = train_knn,
    method = "knn",
    metric = "AUC",
    trControl = tune_control,
    tuneGrid = knn_tune,
)

on.exit(stopCluster(cl))

plotmetrics(knn_cv, "k") +
labs(title = "AUC et Accuracy en fonction des K voisins", x = "Voisins")
```

Visualisons les performances de notre meilleur algorithme des KNN à travers la matrice de confusion et les courbes ROC.

On a vu que l'Accuracy vaut environ $0.57$ pour $k = 4$. Donc en général notre classifieur prédit bien à peu près $6$ infections sur $10$, en tou cas pour `adware`, `ransomeware` et `sms`.  
Il se trompe surtout sur `botnet` où il prédit quand même $29\%$ en `ransomeware`.

```{r}
model_knn <- knn_cv$finalModel
pred_knn <- predict(knn_cv, newdata = test_knn)

confmatrix(pred_knn, test_knn$Label) +
labs(title = "Matrice de confusion KNN, K = 4")
```

Les courbes ROC confirment cette difficulté à classer `botnet`.  
Les infection par `sms` sont les plus facilement discriminables, toutefois le classifieur s'est trompé sur $41\%$ des cas.

```{r message=FALSE, warning=FALSE, fig.width=9.5}
multiroc(pred_knn, test_knn$Label) +
labs(title = "Courbes ROC avec KNN, K = 4")
```

```{r message=FALSE, warning=FALSE}
result <- knn_cv$results %>% 
filter(k == 4)  %>%
mutate(Model = "KNN") %>% 
select(Model, AUC, Accuracy)
```

Nous avons vu que le KNN est un algorithme simple et performant, mais les erreurs de classifications sont assez grandes.  
Passons à une méthode ensembliste, les forêts aléatoires.

## 2. Forêts aléatoires
Les forêts aléatoires se base sur des classifieurs faibles, des arbres de décision $\hat{T}_k$.  
L’idée derrière est que chaque arbre de décision overfit sur une partie des données, tout en possédant une bonne capacité de prédiction. Toute la subtilité réside dans la sélection de l’échantillon à utiliser pour créer chaque arbre, car nous voulons qu’ils soient tous différents. Pour cela, on va bootstraper le jeu de données.

Notre jeu de données contient $M =$ `r dim_X` variables explicatives et l'hyperparamètre qui permettra d'optimiser notre algorithme est $m \leq$ `r dim_X`. Il sera important aussi de réduire $M$ pour ne garder que les variables les plus importantes.

### Approche naïve
Entraînons notre algorithme en passant $m$ sur toute la plage des `r dim_X` variables explicatives et capturons l'importance de chaque variable en fonction de l'impurtée.

```{r message=FALSE, warning=FALSE}
rf_tune_naive <- data.frame(
    mtry = dim_X,
    splitrule = "gini",
    min.node.size = 1
)

cl <- makePSOCKcluster(4)
registerDoParallel(cl)
set.seed(2020)

rf_naive_cv <- train(
    Label ~ .,
    data = train_set,
    method = "ranger",
    metric = "AUC",
    trControl = tune_control,
    tuneGrid = rf_tune_naive,
    importance = 'impurity_corrected'
)

on.exit(stopCluster(cl))
```

Sur nos `r dim_X` variables explicatives, on pourra voir qu'une bonne partie ne porte pas d'information pour la classification des infections en passant par les arbres de décisions.  
En représentant ici uniquement 15% des variables par ordre d'importance, on remarque que très peu semblent avoir du poids.  
Dans notre cas, nous choisirons les variables avant les 5 premiers sauts. En effet, à 4 sauts on ne retiendra que 6 variables, ce qui semble peu et risque l'overfit complet sur nos données d'apprentissage. 

```{r message=FALSE, warning=FALSE, fig.width=9.5}
rf_naive_vars <- rf_naive_cv$finalModel$variable.importance
rf_naive_vars <- tibble(variable = names(rf_naive_vars), importance = rf_naive_vars) %>% 
    arrange(importance) %>% mutate(variable = factor(variable, levels = variable)) %>% 
    top_frac(0.15)

ggplot(rf_naive_vars) + coord_flip() + aes(x = variable, y = importance, fill = importance) + geom_bar(stat = "identity") + 
    labs(title = "15% des variables classées par importance", x = "") +
    theme(plot.title = element_text(hjust = 0.5))
```

Le paramètre $m$ optimal peut changer avec la réduction de variables explicatives, ainsi sa grille de recherche se réduira aussi.
### Optimisation
Sélectionnons donc les 18 variables explicatives les plus importantes.  
Ce qui en fera les données utilisées pour RandomForest.

```{r message=FALSE, warning=FALSE}
data_rf <- data %>% 
select(
    Label, SPort, DPort, IP,
    any_of(tail(rf_naive_vars, 18)$variable)
) %>% 
mutate(
    IP = factor(if_else(
        IP %in% c("172.217", "121", "205.174"),
        as.character(IP), "OtherIP"
    )),
    SPort = factor(if_else(
        SPort %in% c("Private", "Microsoft RPC"),
        as.character(SPort), "OtherSPort"
    )),
    DPort = factor(if_else(
        DPort %in% c("HTTP", "HTTPS"),
        as.character(DPort), "OtherDPort"
    ))
)
```

```{r include=FALSE}
dim_quant_rf <- data_rf %>% select_if(is.numeric) %>% ncol()
dim_IP_rf <- length(unique(data_rf$IP))
dim_SPort_rf <- length(unique(data_rf$SPort))
dim_DPort_rf <- length(unique(data_rf$DPort))
dim_X_rf <- dim_quant_rf + dim_IP_rf-1 + dim_SPort_rf-1 + dim_DPort_rf-1
```

Nous obtenons une dimension de `r dim(data_rf)` après sélection.

Un total de `r dim_X_rf` variables explicatives:

- `r dim_quant_rf` variables quantitatives.
- `r dim_IP_rf` modalités de IP.
- `r dim_SPort_rf` modalités de SPort
- `r dim_DPort_rf` modalités de DPort

Séparons à nouveaux nos échantillons en gardant bien évidemment les index pour avoir exactement la même donnée pour tous les algorithmes mais avec des colonnes en moins.

```{r message=FALSE, warning=FALSE}
train_rf <- split_set(data_rf)[[1]]
test_rf <- split_set(data_rf)[[2]]
```

Entraînons à présent notre algorithme sur les variables sélectionnées en l'optimisant par rapport à $1 \lt m \lt 18$ sur le critère de l'AUC.  
Pour l'AUC, $m$ est optimal à $15$ tandis que l'Accuracy le préfère à $13$.

En faisant les rapport pour $m \in \{13, 15\}$ pour chaque métrique, on remarque que l'Accuracy perd beaucoup alors qu'il n'est pas significatif pour l'AUC.  
On gardera alors $m = 13$ comme paramètre du meilleur algorithme.

```{r message=FALSE, warning=FALSE, fig.width=9.5}
rf_tune <- data.frame(
    mtry = seq(2, 17),
    splitrule = rep("gini", 16),
    min.node.size = rep(1, 16)
)

cl <- makePSOCKcluster(4)
registerDoParallel(cl)
set.seed(2020)

rf_cv <- train(
    Label ~ .,
    data = train_rf,
    method = "ranger",
    metric = "AUC",
    trControl = tune_control,
    tuneGrid = rf_tune
)

on.exit(stopCluster(cl))

plotmetrics(rf_cv, "mtry") +
labs(title = "AUC et Accuracy en fonction des mtry")
```

Sur la matrice de confuion, les erreurs sont nettement plus faibles par rapport à notre KNN, sauf pour `botnet`.  
En effet, notre algorithme a du mal à discriminer `botnet` et `ransomeware` en classifiant $32\%$ de `botnet` en `ransomeware` et $15\%$ l'inverse.

```{r}
model_rf <- ranger(Label~., data = train_rf, mtry = 13, splitrule = "gini", min.node.size = 1)
pred_rf <- predict(model_rf, data = test_rf)$prediction

confmatrix(pred_rf, test_rf$Label) +
labs(title = "Matrice de confusion Random Forest, mtry = 13")
```

Les courbes ROC permettent de visualiser ce fort taux de faux positifs de `ransomeware` par rapport aux autres infections.  
Elles confirment aussi la performance du modèle à bien prédire `adware` vu son taux d'erreur faible dans la matrice de confusion.

```{r message=FALSE, warning=FALSE, fig.width=9.5}
multiroc(pred_rf, test_rf$Label) +
labs(title = "Courbes ROC avec Random Forest, mtry = 13")
```

```{r message=FALSE, warning=FALSE}
result <- result %>%
bind_rows(
    (rf_cv$results %>% 
     filter(mtry == 11)  %>%
     mutate(Model = "RF") %>% 
     select(Model, AUC, Accuracy)
    )
)
```

On a vu que le modèle des forêts aléatoires est vraiment puissant en combinant plusieurs arbres de décisions qui sont des classifieurs faibles.  
En restant dans ce thème de modèles d'ensembles, nous testerons l'algorithme de Logitboost.
## 3. Logit Boost
L'algorithme logitboost utilise le gradient boosting pour la classification. Il s'agit donc d'un cas parmi les algorithme de descentes de gradient que nous allons aussi tester dans la suite.

L'algorithme applique la fonction coût de la régression logistique, en itérant.  
Le paramètre à chercher sera donc le nombre d'itération $m$.

### Approche Naïve
Entraînons l'algorithme sur une grille allant de 1 à 170 par pas de 10.

Pour une unique itération, on a un modèle avec un Accuracy élevé mais un AUC très faible. Toutefois, il s'agit ici de l'aléatoire. On considère les courbes à partir de 10 itérations.  
L'AUC et l'Accuracy s'accorde à un meilleur modèle pour $m = 130$.  

```{r message=FALSE, warning=FALSE, fig.width=9.5}
lgb_tune_naive <- data.frame(
    nIter = seq(1, 171, by =  10)
)

cl <- makePSOCKcluster(4)
registerDoParallel(cl)
set.seed(2020)

lgb_cv_naive <- train(
    Label ~ .,
    data = train_set,
    method = "LogitBoost",
    metric = "AUC",
    trControl = tune_control,
    tuneGrid = lgb_tune_naive
)

on.exit(stopCluster(cl))

plotmetrics(lgb_cv_naive, "nIter") +
labs(title = "AUC et Accuracy en fonction des nIter")
```

On optimisera pour $m \in [120, 140]$

### Optimisation
Il n'y a pas de traitement à faire en plus sur les données puisqu'il n'y a pas sélection de variable.  
Donc en divisera les mêmes échantillons.

```{r message=FALSE, warning=FALSE}
train_lgb <- split_set(data)[[1]]
test_lgb <- split_set(data)[[2]]
```

En entraînant sur la nouvelle plage $[120, 140]$.  
On notera que lors d'un pré-entraînement, l'algorithme a été très sensible au fait que le nombre d'itération soit pair ou impair. De meilleurs performances ont été constatées sur nos deux critères en testants sur des itérations impaires. La grille sera alors $[119, 141]$ par pas de 2.  
Nos deux critères s'accorde sur un même $m = 129$ itérations comme meilleur algorithme.  

```{r message=FALSE, warning=FALSE, fig.width=9.5}
lgb_tune <- data.frame(
    nIter = seq(119, 141, by = 2)
)

cl <- makePSOCKcluster(4)
registerDoParallel(cl)
set.seed(2020)

lgb_cv <- train(
    Label ~ .,
    data = train_lgb,
    method = "LogitBoost",
    metric = "AUC",
    trControl = tune_control,
    tuneGrid = lgb_tune
)

on.exit(stopCluster(cl))

plotmetrics(lgb_cv, "nIter") +
labs(title = "AUC et Accuracy en fonction des nIter")
```

On s'aperçoit, en analysant les performances, que notre modèle Logitboost est moins bon que notre modèle Random forest précédent.  
Il effectue certes moins d'erreur que le KNN, mais ne classe pas le tiers de nos données pour chaque label.

```{r}
model_lgb <- lgb_cv$finalModel
pred_lgb <- predict(lgb_cv, newdata = test_lgb)

confmatrix(pred_lgb, test_lgb$Label) +
labs(title = "Matrice de confusion Logitboost, nIter = 129")
```

```{r message=FALSE, warning=FALSE, fig.width=9.5}
multiroc(pred_lgb, test_lgb$Label) +
labs(title = "Courbes ROC avec Logitboost, nIter = 129")
```

```{r message=FALSE, warning=FALSE}
result <- result %>%
bind_rows(
    (lgb_cv$results %>% 
     filter(nIter == 129)  %>%
     mutate(Model = "LGB") %>% 
     select(Model, AUC, Accuracy)
    )
)
```

On a vu que notre modèle Logitbost n'est pas plus performant que les deux premiers sur ce problème.  
Nous testerons un dernier algorithme, lui aussi basé sur le boosting.
## 4. Gradient boosting
Le Gradient boosting utilise le gradient de la fonction de perte pour le calcul des poids des individus lors de la construction de chaque nouveau modèle.  
L'augmentation du nombre d'arbre $k$ n'augmente pas pour autant le risque d'overfitting tant que la profondeur $p$ de celles-ci est petite. Aussi, la réduction du nombre de variables explicatives permet d'améliorer les arbres construites.

### Approche naïve
Nous entraînerons donc sur ces deux paramètres pour trouver notre meilleur modèle tout en capturant l'importance des variables, avec:

$$\left\{
    \begin{array}{ll}
        p \in \{1, 3, 5\} \\
        k \in \{50, 100, 500, 1000, 1500, 2500, 4000\}
    \end{array}
\right.
$$

On remarque que l'AUC est meilleure pour $p \gt 1$. Pour $p = 5$, l'AUC est maximale pour $k = 500$.  
La meilleure AUC pour cette recherche naïve est atteinte pour $p = 3$ et $k \in [1000, 2500]$. Nous choisirons l'option où il y a le plus d'arbre avec une profondeur plus faible pourvu que cela améliore les performances tout en réduisant le risque d'overfitting, malgré que l'apprentissage demande plus de temps de calcul donc.

```{r}
gbm_tune_naive <- data.frame(
    shrinkage = rep(0.1, 21),
    interaction.depth = rep(c(1, 3, 5), 7),
    n.minobsinnode = rep(10, 21),
    n.trees = c(rep(50, 3), rep(100, 3), 
                rep(500, 3), rep(1000, 3),
                rep(1500, 3), rep(2500, 3),
                rep(4000, 3)))

cl <- makePSOCKcluster(4)
registerDoParallel(cl)
set.seed(2020)

gbm_cv_naive <- train(
    Label ~ .,
    data = train_set,
    method = "gbm",
    metric = "AUC",
    trControl = tune_control,
    tuneGrid = gbm_tune_naive,
    verbose = FALSE
)

on.exit(stopCluster(cl))

ggplot(gbm_cv_naive) +
labs(title = "AUC pour chaque nombre de division en fonction du nombre d'arbres")
```

Du côté des variables explicatives, on a l'importance moyenne pour chaque variable. C'est-à-dire la moyenne de l'importance par classe par variable.  
Si nous visualisons la totalité, sur les `r dim_X`, près de la moitié a une importance moyenne nulle ou négative.  
Parmi les 20% les plus importantes, nous ne garderons que celles avant les 3 sauts, en l'occurence celles qui ont une importance moyenne supérieure à 2.

```{r}
gbm_naive_vars <- summary(gbm_cv_naive$finalModel, plotit = F) %>% as_tibble() %>% 
arrange(rel.inf) %>% mutate(var = factor(var, levels = var)) %>% top_frac(0.20)

ggplot(gbm_naive_vars) + coord_flip() + aes(x = var, y = rel.inf, fill = rel.inf) + geom_bar(stat = "identity") + 
    labs(title = "20% des variables classées par importance", x = "", y = "Importance moyenne par classe") +
    theme(plot.title = element_text(hjust = 0.5)) + guides(fill = F)
```

Nous optimisons donc sur $p = 3$, sur une nouvelle plage de $k$ et de variables explicatives réduites.
### Optimisation
Mettons en forme nos 13 variables explicatives et séparons cette nouvelle table en deux échantillons avec toujours les mêms index que les autres algorithmes.

```{r}
data_gbm <- data %>% 
select(
    Label, DPort, IP,
    any_of(tail(gbm_naive_vars, 13)$var)
) %>% 
mutate(
    IP = factor(if_else(
        as.character(IP) == "172.217",
        as.character(IP), "OtherIP"
    )),
    DPort = factor(if_else(
        DPort == "HTTPS",
        as.character(DPort), "OtherDPort"
    ))
)
```

```{r}
dim_quant_gbm <- data_gbm %>% select_if(is.numeric) %>% ncol()
dim_IP_gbm <- length(unique(data_gbm$IP))
dim_DPort_gbm <- length(unique(data_gbm$DPort))
dim_X_gbm <- dim_quant_gbm + dim_IP_gbm-1 + dim_DPort_gbm-1
```

Nous obtenons une dimension de `r dim(data_gbm)` après sélection.

Un total de `r dim_X_gbm` variables explicatives:

- `r dim_quant_gbm` variables quantitatives.
- `r dim_IP_gbm` modalités de IP.
- `r dim_DPort_gbm` modalités de DPort

Séparons à nouveaux nos échantillons en gardant bien évidemment les index pour avoir exactement la même donnée pour tous les algorithmes mais avec des colonnes en moins.

```{r}
train_gbm <- split_set(data_gbm)[[1]]
test_gbm <- split_set(data_gbm)[[2]]
```

Entraînons à présent notre algorithme sur les variables sélectionnées en l'optimisant par rapport à $k \in [1100, 2400]$ par pas de 100 sur le critère de l'AUC.  
Pour l'AUC, $k$ est optimal à $1800$ tandis que l'Accuracy le préfère à $1100$.

A $k = 1100$, compte tenu de l'AUC, l'algorithme n'a pas encore convergé.  
A $k = 1800$, on retrouve l'AUC maximale ainsi qu'un maximum local pour l'Accuracy. On prefèrera alors garder $k = 1800$

```{r}
gbm_tune <- data.frame(
    shrinkage = rep(0.1, 14),
    interaction.depth = rep(3, 14),
    n.minobsinnode = rep(10, 14),
    n.trees = seq(1100, 2400, by = 100)
)

cl <- makePSOCKcluster(4)
registerDoParallel(cl)
set.seed(2020)

gbm_cv <- train(
    Label ~ .,
    data = train_gbm,
    method = "gbm",
    metric = "AUC",
    trControl = tune_control,
    tuneGrid = gbm_tune,
    verbose = FALSE
)

on.exit(stopCluster(cl))

plotmetrics(gbm_cv, "n.trees") +
labs(title = "AUC et Accuracy en fonction du nombre d'arbres")
```

En visualisant les performances de notre meilleur modèle, on constate que ce dernier réussit le mieux à bien classer `botnet` et `ransomeware` par rapport aux 3 premiers en ayant un taux d'erreur d'une différence de 10%. Il a par contre une erreur un peu plus grande sur la classification entre `adware` et `sms` par rapport à notre modèle Random Forest.

```{r}
model_gbm <- gbm_cv$finalModel
pred_gbm <- predict(gbm_cv, newdata = test_gbm)

confmatrix(pred_gbm, test_gbm$Label) +
labs(title = "Matrice de confusion Stochastic gradient boosting, n.trees = 1800")
```

Les courbes ROC confirme la meilleure gestion du `ransomeware` par le gradient boosting.  
Elles semblent s'uniformiser pour chaque classification des infections.

```{r}
multiroc(pred_gbm, test_gbm$Label) +
labs(title = "Courbes ROC avec Stochastic gradient boosting, n.trees = 1800")
```

```{r}
result <- result %>%
bind_rows(
    (gbm_cv$results %>% 
     filter(n.trees == 1800)  %>%
     mutate(Model = "GBM") %>% 
     select(Model, AUC, Accuracy)
    )
)
```

Notre modèle de Gradient boosting semble donc être prometteur pour ce problème.

A présent, comparons nos différents modèles pour choisir le meilleur (ou le plus adapté).
# IV. Sélection de modèle

## 1. Comparaisons
Nous nous sommes fixé comme métriques l'AUC (ROC) et l'Accuracy pour notre règle de décision.

Une première comparaison est de visualiser, avec les courbes ROC, la performance de chaque modèle avec qu'on a optimisé pour classifier chaque infection contre les autres.  
On note directement la compétition entre Random forest et Gradient boosting qui sont au coudes-à-coudes.  
Pour classer `botnet` et `sms`, les deux modèles sont aussi performant que l'autre et sont d'ailleurs les meilleurs modèles pour classifier ces deux infections. Le Gradient boosting s'illustre dans sa performance pour classifier `ransomeware` par rapport aux 3 autres algorithmes qui se confondent. Random forest prend le dessus sur Gradient boosting dans sa capacité à bien classifier `adware`. 

D'autre part, Logitboost est meilleur que le KNN pour classifier les `botnet`, et inversement pour les `adware`.

```{r}
predicted <- tibble(Y = test_set$Label, KNN = pred_knn, RF = pred_rf, LGB = pred_lgb, GBM = pred_gbm)
plotlist <- vector(mode = "list")
for (label in unique(data$Label)){
    roclist <- vector(mode = "list")
    one_rest <- predicted %>% 
    mutate_all(funs(
        if_else(. == !!label, 1, 0)
    ))
    for (model in c('KNN', 'RF', 'LGB', 'GBM')){
        roclist[[model]] <- roc(pull(one_rest, model), one_rest$Y)
    }
    plotlist[[label]] <- ggroc(roclist, aes = "color", size = 1.2, legacy.axes = TRUE) +
    labs(color = "Models", title = label, x = NULL, y = NULL) + geom_abline() +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"))
}
```

```{r}
ggarrange(
    plotlist = plotlist,
    ncol = 2, nrow = 2,
    common.legend = T,
    legend = "right"
) %>% 
annotate_figure(
    top = "Courbes ROC des modèles pour chaque label",
    left = "sensitivity",
    bottom = "1 - specificity"
)
```

<!-- #region -->
L'analyse des valeurs d'AUC et d'Accuracy devrait donc nous aider à trancher entre nos deux meilleurs modèles.


Avec 
$\left\{
    \begin{array}{ll}
        \text{AUC}_\text{RF} = 0.87 \\
        \text{Accuracy}_\text{RF} = 0.65
    \end{array}
\right.$ 
et 
$\left\{
    \begin{array}{ll}
        \text{AUC}_\text{GBM} = 0.85 \\
        \text{Accuracy}_\text{GBM} = 0.62
    \end{array}
\right.$
le modèle **Random Forest l'emporte** sur les deux métriques. La différence de scores n'est presque pas significative entre les deux modèles.  
D'un point de vue métier, le poids de l'erreur de classification d'une infection par rapport à une autre affecte grandement la prise de décision.

Par exemple, dans une entreprise où les appareils recoivent des requêtes externes mais n'en renvoient pas, une infection par `botnet` est bénigne et donc sera ignorée alors qu'un `ransomeware` qui chiffre petit à petit les données serait fatal. Dans ce cas, on préfèrera le modèle de Gradient Boosting qui réduit l'erreur de classification entre ces deux infections.
<!-- #endregion -->

```{r message=FALSE, warning=FALSE, fig.width=9.5, fig.height=3}
result %>% 
gather(metric, value, -Model) %>% 
ggplot() +
aes(fill = metric, y = value, x = Model) +
geom_bar(position = "dodge", stat = "identity") +
coord_flip(ylim = c(0, 1)) +
labs(title = "Comparaison des modèles") +
theme(plot.title = element_text(hjust = 0.5))
```

Compte tenu de ces différentes analyses, nous retiendrons dans le cadre de ce document le modèle sur les forêts aléatoires.

## 2. Pipeline
Comme nous avons retenu les modèle Random Forest, nous revenons sur la structure de ses données `data_rf`, `train_rf` et `test_rf`.

Pour pouvoir être interprétées par notre modèle et effectuer une préduction les nouvelles données doivent avoir:

- la colonne `SPort` qui doit être factorisé selon les attribution de l'IANA et avoir soit "Private", soit "Microsoft RPC" ou autre
- la colonne `DPort` qui doit être factorisé selon les attribution de l'IANA et avoir soit "HTTPS", soit "HTTP" ou autre
- la colonne `IP` qui doit être splité en Net-ID et doit avoir soit "121", "172.217", "205.174" ou autre
- les 11 colonnes quantitatives qui doivent être respectivement standardisées par les moyennes et variances de `Flow.Duration`, `Flow.Packets.s`, `Flow.IAT.Mean`, `Flow.IAT.Max`, `Flow.IAT.Min`, `Fwd.IAT.Mean`, `Fwd.IAT.Min`, `Fwd.Packets.s`, `Min.Packet.Length`, `Init_Win_bytes_forward` et `Init_Win_bytes_backward`, dans cet ordre.
